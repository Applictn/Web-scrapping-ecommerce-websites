{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from flask import Flask, render_template, request\n",
    "import os\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mobile_prices():\n",
    "    \n",
    "    \n",
    "    # user input of mobile\n",
    "#     def __init__(self):\n",
    "#         self.name = input(\"Enter Mobile Name here: \")\n",
    "        \n",
    "        \n",
    "    # scraping Flipkart\n",
    "    def flipkart(self, product_name):\n",
    "        url = \"https://www.flipkart.com/\"\n",
    "        query = \"search?q=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        site = 'Flipkart'\n",
    "        \n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.content, 'html.parser')\n",
    "        \n",
    "        self.flipkart_details = []\n",
    "        if soup.find_all(class_='_31qSD5'):\n",
    "            for i,mob in enumerate(soup.find_all(class_ = '_31qSD5')):\n",
    "                try:\n",
    "                    name = mob.find(class_ = '_3wU53n').text.strip()\n",
    "                    price = mob.find(class_ = '_1vC4OE _2rQ-NK').text.strip()\n",
    "                    try:\n",
    "                        img_det = re.findall(\"keySpecs(.*?)jpeg\", result.text)[i]\n",
    "                        details = re.findall(\"\\[\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\".*url\\\":\\\"(.*)\", img_det)[0]\n",
    "                        url = details[5]\n",
    "                        url = re.sub(\"{@width}|{@height}\", '250', url) + 'jpeg'\n",
    "                    except:\n",
    "                        url = ''\n",
    "                    try:\n",
    "                        prod_url = mob.attrs['href']\n",
    "                        prod_url = \"https://www.flipkart.com\" + prod_url\n",
    "                    except:\n",
    "                        prod_url = ''\n",
    "                    try:\n",
    "                        rating = mob.find('div', class_ = 'hGSR34 _2beYZw').text.strip()\n",
    "                    except:\n",
    "                        rating = ''\n",
    "                    try:\n",
    "                        no_of_ratings = re.findall('(.*)Ratings',mob.find_all('span', class_ = '_38sUEc')[0].text)[0].strip()\n",
    "                    except:\n",
    "                        no_of_ratings = ''\n",
    "    #                 no_of_reviews = re.findall('\\xa0&\\xa0(.*)Reviews',mob.find_all('span', class_ = '_38sUEc')[0].text)[0].strip()\n",
    "                    self.flipkart_details.append([name, price, rating, no_of_ratings, site, url, prod_url])\n",
    "#                     print(site, name, price, url, prod_url)\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            for i,mob in enumerate(soup.find_all('div', class_='_3liAhj _1R0K0g')):\n",
    "                try:\n",
    "                    name = mob.find(class_ = '_2cLu-l').text.strip()\n",
    "                    price = mob.find(class_ = '_1vC4OE').text.strip()\n",
    "                    try:\n",
    "                        img_det = re.findall(\"keySpecs(.*?)jpeg\", result.text)[i]\n",
    "                        details = re.findall(\"\\[\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\",\\\"(.*?)\\\".*url\\\":\\\"(.*)\", img_det)[0]\n",
    "                        url = details[5]\n",
    "                        url = re.sub(\"{@width}|{@height}\", '250', url) + 'jpeg'\n",
    "                    except:\n",
    "                        url = ''\n",
    "                    try:\n",
    "                        prod_url = mob.find(class_ = 'Zhf2z-')\n",
    "                        prod_url = prod_url.attrs['href']\n",
    "                        prod_url = \"https://www.flipkart.com\" + prod_url \n",
    "                    except:\n",
    "                        prod_url = ''\n",
    "                    try:\n",
    "                        rating = mob.find(class_ = 'hGSR34 _2beYZw').text.strip()\n",
    "                    except:\n",
    "                        rating = ''\n",
    "                    try:\n",
    "                        no_of_ratings = mob.find(class_ = '_38sUEc').text.strip('()')\n",
    "                    except:\n",
    "                        no_of_ratings = ''\n",
    "                    self.flipkart_details.append([name, price, rating, no_of_ratings, site, url, prod_url])\n",
    "#                     print(site, name, price, url, prod_url)\n",
    "                except:\n",
    "                    pass\n",
    "        return True\n",
    "                \n",
    "    # scraping Snapdeal    \n",
    "    def snapdeal(self, product_name):\n",
    "        url = \"https://www.snapdeal.com/\"\n",
    "        query = \"search?keyword=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        site = 'Snapdeal'\n",
    "        \n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.content, 'html.parser')\n",
    "        \n",
    "        self.snapdeal_details = []\n",
    "        for mob in soup.find_all(class_='col-xs-6'):\n",
    "            try:\n",
    "                name = mob.find(class_ = 'product-title').text.strip()\n",
    "                price = mob.find(class_ = 'lfloat product-price').text.strip()\n",
    "                try:\n",
    "                    prod_url = mob.find(class_ = 'dp-widget-link')\n",
    "                    prod_url = prod_url.attrs['href']\n",
    "                except:\n",
    "                    prod_url = ''\n",
    "                try:\n",
    "                    rating = re.findall('width:(.*?)\">',str(mob.find(class_ = 'filled-stars')))[0]\n",
    "                except:\n",
    "                    rating = ''\n",
    "                try:\n",
    "                    no_of_ratings = mob.find(class_ = 'product-rating-count').text.strip('()')\n",
    "                except:\n",
    "                    no_of_ratings = ''\n",
    "                try:\n",
    "                    url = mob.find(class_ = 'product-image')\n",
    "                    url = url.attrs['srcset']\n",
    "                except:\n",
    "                    url = ''\n",
    "                self.snapdeal_details.append([name, price, rating, no_of_ratings, site, url, prod_url])\n",
    "#                 print(site, name, price, prod_url)\n",
    "            except:\n",
    "                pass\n",
    "        return True\n",
    "            \n",
    "    # scraping PaytmMall    \n",
    "    def paytmmall(self, product_name):\n",
    "        url = \"https://paytmmall.com/\"\n",
    "        query = \"shop/search?q=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.content, 'html.parser')\n",
    "        \n",
    "        site = 'PaytmMall'\n",
    "        \n",
    "        self.paytmmall_details = []\n",
    "        for mob in soup.find_all('div', class_='_3WhJ'):\n",
    "            try:\n",
    "                name = mob.find(class_ = '_2apC').text.strip()\n",
    "                price = mob.find(class_ = '_1kMS').text.strip()\n",
    "                try:\n",
    "                    prod_url = mob.find('a')\n",
    "                    prod_url = prod_url.attrs['href']\n",
    "                    prod_url = \"https://paytmmall.com\" + prod_url\n",
    "                except:\n",
    "                    prod_url = ''\n",
    "                try:\n",
    "                    cashback = mob.find(class_ = '_27VV').text.strip()\n",
    "                except:\n",
    "                    cashback = ''\n",
    "                try:\n",
    "                    url = mob.find('img')\n",
    "                    url = url.attrs['src']\n",
    "                except:\n",
    "                    url = ''\n",
    "                self.paytmmall_details.append([name, price, cashback, site, url, prod_url])\n",
    "#                 print(site, name, price, prod_url)\n",
    "            except:\n",
    "                pass\n",
    "        return True\n",
    "    \n",
    "    # scraping ShopClues\n",
    "    def shopclues(self, product_name):\n",
    "        url = \"https://www.shopclues.com/\"\n",
    "        query = \"search?q=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        site = 'Shopclues'\n",
    "        \n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.content, 'html.parser')\n",
    "        \n",
    "        self.shopclues_details = []\n",
    "        for mob in soup.find_all('div', class_='column col3 search_blocks'):\n",
    "            try:\n",
    "                name = mob.find('h2').text.strip()\n",
    "                price = mob.find(class_ = 'p_price').text.strip()\n",
    "                try:\n",
    "                    prod_url = mob.find('a')\n",
    "                    prod_url = prod_url.attrs['href']\n",
    "                    prod_url = \"http:\" + prod_url\n",
    "                except:\n",
    "                    prod_url = ''\n",
    "                try:\n",
    "                    url = mob.find('img')\n",
    "                    url = url.attrs['data-img']\n",
    "                except:\n",
    "                    url = ''\n",
    "                self.shopclues_details.append([name, price, site, url, prod_url])\n",
    "#                 print(site, name, price, prod_url)\n",
    "            except:\n",
    "                pass\n",
    "        return True\n",
    "            \n",
    "    # scraping TataCliq\n",
    "    def tatacliq(self, product_name):\n",
    "        url = \"https://www.tatacliq.com/\"\n",
    "        query = \"search/?text=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        site = 'TataCliq'\n",
    "        \n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "#         options.add_argument(\"--window-size=1000,600\")\n",
    "\n",
    "        # download chromedriver form internet for Chrome selenium and give the path here\n",
    "        CHROMEDRIVER_PATH = 'D:\\chromedriver'\n",
    "        driver = webdriver.Chrome(CHROMEDRIVER_PATH, chrome_options=options)\n",
    "        driver.get(url)\n",
    "        \n",
    "        self.tatacliq_details = []\n",
    "        for mob in driver.find_elements_by_class_name('LK_htgvFpS2PUMylIQlif'):\n",
    "            try:\n",
    "                name = mob.find_elements_by_tag_name('h3')[1].text.strip()\n",
    "                price = mob.find_elements_by_tag_name('h3')[2].text.strip()\n",
    "                self.tatacliq_details.append([name, price, site])\n",
    "#                 print(site, name, price)\n",
    "            except:\n",
    "                pass\n",
    "        return True\n",
    "    \n",
    "    # scraping Amazon    \n",
    "    def amazon(self, product_name):\n",
    "#         url = \"https://www.amazon.in/\"\n",
    "        \n",
    "        site = 'Amazon'\n",
    "\n",
    "        url = \"https://www.amazon.in/\"\n",
    "        query = \"s/ref=nb_sb_noss_2?url=search-alias%3Daps&field-keywords=\" + product_name\n",
    "        url = url + query\n",
    "        \n",
    "        header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "        r = requests.get(url, headers = header)\n",
    "        \n",
    "        driver = BeautifulSoup(r.content)\n",
    "        self.amazon_details = []\n",
    "        \n",
    "        for mob in driver.find_all(class_ = 's-item-container'):\n",
    "            try:\n",
    "                name = mob.find('h2', class_ = 'a-size-base').text.strip()\n",
    "                price = mob.find('span', class_ = 'a-size-base').text.strip()\n",
    "                try:\n",
    "                    prod_url = mob.find(class_ = 'a-link-normal')\n",
    "                    prod_url = prod_url.attrs['href']\n",
    "                    if re.search('url=https', prod_url):\n",
    "                        prod_url = \"https://www.amazon.in\" + prod_url\n",
    "                except:\n",
    "                    prod_url = ''\n",
    "                try:\n",
    "                    url = mob.find('img')\n",
    "                    url = url.attrs['src']\n",
    "                except:\n",
    "                    url = ''\n",
    "                self.amazon_details.append([name, price, site, url, prod_url])\n",
    "#                 print(site, name, price, prod_url)\n",
    "            except:\n",
    "                try:\n",
    "                    name = mob.find('h2', class_ = 'a-size-medium').text.strip()\n",
    "                    price = mob.find('span', class_ = 'a-size-base').text.strip()\n",
    "                    try:\n",
    "                        prod_url = mob.find(class_ = 'a-link-normal')\n",
    "                        prod_url = prod_url.attrs['href']\n",
    "                        if re.search('url=https', prod_url):\n",
    "                            prod_url = \"https://www.amazon.in\" + prod_url\n",
    "                    except:\n",
    "                        prod_url = ''\n",
    "                    try:\n",
    "                        url = mob.find('img')\n",
    "                        url = url.attrs['src']\n",
    "                    except:\n",
    "                        url = ''\n",
    "                    self.amazon_details.append([name, price, site, url, prod_url])\n",
    "#                     print(site, name, price, prod_url)\n",
    "                except:\n",
    "                    pass\n",
    "        return True\n",
    "    \n",
    "    def fonearena(self):\n",
    "        self.fone=[]\n",
    "        data= requests.get(\"https://www.fonearena.com/\")\n",
    "        soup= BeautifulSoup (data.text, 'lxml')\n",
    "        table = soup.find_all('div',{'id':'news'})\n",
    "\n",
    "        for m in table[0].find_all('h3'):\n",
    "            name=m.text\n",
    "            url=m.find('a').get('href')\n",
    "            self.fone.append([name,url])\n",
    "        return self.fone\n",
    "    \n",
    "    def phonearena(self):\n",
    "        self.phonear=[]\n",
    "        data= requests.get(\"https://www.phonearena.com/\")\n",
    "        soup= BeautifulSoup (data.text, 'lxml')\n",
    "        table = soup.find_all('div',{'class':'ln-item'})\n",
    "\n",
    "        site=\"https://www.phonearena.com/\"\n",
    "\n",
    "        for m in table:\n",
    "            name=m.text\n",
    "            url=site + m.find('a').get('href')\n",
    "            self.phonear.append([name,url])\n",
    "        return self.phonear\n",
    "    \n",
    "    \n",
    "    def youtube(self, gadjet):\n",
    "        base = \"https://www.youtube.com/results?search_query=\" + gadjet\n",
    "\n",
    "        r = requests.get(base)\n",
    "\n",
    "        page = r.text\n",
    "        soup=BeautifulSoup(page,'html.parser')\n",
    "        vids = soup.findAll('a',attrs={'class':'yt-uix-tile-link'})\n",
    "        site=\"https://www.youtube.com/\"\n",
    "        self.yout=[]\n",
    "        \n",
    "        try:\n",
    "            for a in soup.find_all('div', class_ = 'yt-lockup-content'):\n",
    "                att = a.find('a').attrs\n",
    "                name=att['title']\n",
    "                url=re.sub(\"watch.v=\",\"embed/\",att['href'])\n",
    "                \n",
    "                url=site + url\n",
    "                self.yout.append([name,url])\n",
    "        except:\n",
    "            return \"\"\n",
    "        return self.yout\n",
    "        \n",
    "    \n",
    "    # combining and filtering all the scraped results\n",
    "    def combine(self, product_name):\n",
    "        \n",
    "        # regex to remove products containing these keywords\n",
    "        reg = 'cover|case|Refurbished|guard|glass|defender|stand|compatible|combo|accessory|headphone|headset|\\sac\\sRefrigerator|washing|keyboard|door|monitor|inverter|machine|kettle|coin|bag|sandal|cable|plug|back'\n",
    "        amazon = pd.DataFrame(self.amazon_details, columns= ['Name', 'Price', 'Site', 'Url', 'Product_Url'])\n",
    "#         tatacliq = pd.DataFrame(self.tatacliq_details, columns= ['Name', 'Price', 'Site'])\n",
    "        shopclues = pd.DataFrame(self.shopclues_details, columns= ['Name', 'Price', 'Site', 'Url', 'Product_Url'])\n",
    "        paytmmall = pd.DataFrame(self.paytmmall_details, columns= ['Name', 'Price', 'Cashback', 'Site', 'Url', 'Product_Url'])\n",
    "        snapdeal = pd.DataFrame(self.snapdeal_details, columns= ['Name', 'Price', 'Rating', 'No of Ratings', 'Site', 'Url', 'Product_Url'])\n",
    "        flipkart = pd.DataFrame(self.flipkart_details, columns= ['Name', 'Price', 'Rating', 'No of Ratings', 'Site', 'Url', 'Product_Url'])\n",
    "#         data = pd.concat([amazon , tatacliq , shopclues, paytmmall, snapdeal, flipkart])\n",
    "        data = pd.concat([amazon, shopclues, paytmmall, snapdeal, flipkart])\n",
    "        \n",
    "        # converting price into string\n",
    "        data.Price = data.Price.astype(str)\n",
    "        \n",
    "        # removing unnecessary symbols from price\n",
    "        data.Price = data.Price.str.replace('₹|Rs.?|,|\\..+','', case = False)\n",
    "        data.Cashback = data.Cashback.str.replace('₹|Rs.?|,|.ashback|\\..+','', case = False)\n",
    "        \n",
    "        # removing products containing certain keywords\n",
    "        data = data[~data.Name.str.contains(reg, case = False)]\n",
    "        \n",
    "        # only keeping products containing our original query\n",
    "        data = data[data.Name.str.contains(product_name, case = False)]\n",
    "        \n",
    "        # converting price into integer from string\n",
    "        data.Price = data.Price.astype(int)\n",
    "        \n",
    "        # removing products having price less than the mean-2*std\n",
    "        data = data[~(data.Price < data.Price.mean() - data.Price.mean()/2)]\n",
    "\n",
    "        data.sort_values(['Price'], inplace = True)\n",
    "        data = data.head(1)\n",
    "        ans = \"{} on {} for Rs. {}\".format(data.Name.iloc[0], data.Site.iloc[0], str(data.Price.iloc[0]))\n",
    "        img = data.Url.iloc[0]\n",
    "        url = data.Product_Url.iloc[0]\n",
    "        try:\n",
    "            header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "            res = requests.get(url, headers = header)\n",
    "            soup = BeautifulSoup(res.content)\n",
    "            details = []\n",
    "            if re.search('amazon', url):\n",
    "                try:\n",
    "                    a = soup.find(id = 'feature-bullets')\n",
    "                    for det in a.find_all('span'):\n",
    "                        details.append(det.text.strip())\n",
    "                    details = details[:-1]\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            elif re.search('flipkart', url):\n",
    "                try:\n",
    "                    a = soup.find(class_ = '_3WHvuP')\n",
    "                    for det in a.find_all('li'):\n",
    "                        details.append(det.text.strip())\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            elif re.search('snapdeal', url):\n",
    "                try:\n",
    "                    a = soup.find(class_ = 'spec-body')\n",
    "                    for det in a.find_all(class_ = 'h-content'):\n",
    "                        details.append(det.text.strip())\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            elif re.search('shopclues', url):\n",
    "                try:\n",
    "                    a = soup.find(class_ = 'des_info')\n",
    "                    for det in a.find_all('li'):\n",
    "                        details.append(det.text.strip())\n",
    "                    details = details[:-1]\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            elif re.search('paytmmall', url):\n",
    "                try:\n",
    "                    a = soup.find(class_ = 'wJuG _1CXW')\n",
    "                    for det_1, det_2 in zip(a.find_all(class_ = '_2LOI'), a.find_all(class_ = \"w3LC\")):\n",
    "                        details.append(det_2.text + ': ' + det_1.text)\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            details=[]\n",
    "        answer = [img, ans, url,details]\n",
    "#         ans = [data.Name.iloc[0], data.Site.iloc[0], str(data.Price.iloc[0])]\n",
    "\n",
    "\n",
    "        \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj = mobile_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'D:') # directory of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [07/Feb/2019 13:28:36] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Feb/2019 13:28:36] \"GET /static/home_slider_1.jpg HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samsung s9\n",
      "samsung s9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Feb/2019 13:29:27] \"POST /hello HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Feb/2019 13:29:27] \"GET /static/home_slider_1.jpg HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samsung s9-apple iphone x-lg g7\n",
      "samsung s9\n",
      "apple iphone x\n",
      "lg g7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [07/Feb/2019 13:31:41] \"POST /hello HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Feb/2019 13:31:42] \"GET /static/home_slider_1.jpg HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [07/Feb/2019 13:32:49] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Feb/2019 13:32:50] \"GET /static/home_slider_1.jpg HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "#app = Flask(__name__)\n",
    "app = Flask(__name__, static_url_path = '/static')\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    fon=obj.fonearena()[:10]\n",
    "    pho=obj.phonearena()[:5]\n",
    "    \n",
    "    return render_template(\"index.html\", name=fon, phone=pho)\n",
    "\n",
    "@app.route(\"/hello\", methods=[\"POST\"])\n",
    "def hello():\n",
    "    question = request.form.get(\"name\")\n",
    "    print(question)\n",
    "    ques = question.split('-')\n",
    "    ans = []\n",
    "    for name in ques:\n",
    "        name = name.strip()\n",
    "        print(name)\n",
    "        obj.flipkart(name)\n",
    "        obj.snapdeal(name)\n",
    "        obj.shopclues(name)\n",
    "        obj.paytmmall(name)\n",
    "        obj.amazon(name)\n",
    "        \n",
    "#         obj.tatacliq(name)\n",
    "        ans.append(obj.combine(name))\n",
    "    yo=obj.youtube(question)[:4]\n",
    "    return render_template(\"hello.html\", name = ans, youtu=yo)\n",
    "    \n",
    "    \n",
    "\n",
    "# @app.route(\"/more\")\n",
    "# def more():\n",
    "#     print(an2)\n",
    "#     return render_template(\"more.html\",name=an2)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
